{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9fb2df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "import ujson\n",
    "from random import randint\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9127f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete files if present\n",
    "try:\n",
    "    os.remove('Authors_URL.txt')\n",
    "    os.remove('scraper_results.json')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "def write_authors(list1, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        for i in range(0, len(list1)):\n",
    "            f.write(list1[i] + '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1735e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_7468\\1389704327.py:7: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n",
      "  webOpt.headless = True\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_7468\\1389704327.py:8: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(), options=webOpt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawler has begun...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7468\\1389704327.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m \u001b[0minitCrawlerScraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7468\\1389704327.py\u001b[0m in \u001b[0;36minitCrawlerScraper\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Click on Next button to visit next page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arguments[0].click();\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_link_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Crawler has found\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pureportal profiles\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def initCrawlerScraper(seed):\n",
    "    # Initialize driver for Chrome\n",
    "    webOpt = webdriver.ChromeOptions()\n",
    "    webOpt.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    webOpt.add_argument('--ignore-certificate-errors')\n",
    "    webOpt.add_argument('--incognito')\n",
    "    webOpt.headless = True\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=webOpt)\n",
    "    driver.get(seed)  # Start with the original link\n",
    "    \n",
    "    \n",
    "    Links = []  # Array with pureportal profiles URL\n",
    "    pub_data = []  # To store publication information for each pureportal profile\n",
    "\n",
    "    print(\"Crawler has begun...\")\n",
    "    while True:\n",
    "        page = driver.page_source\n",
    "        # XML parser to parse each URL\n",
    "        bs = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "        # Extracting exact URL by splitting string into list\n",
    "        for link in bs.findAll('a', class_='link person'):\n",
    "            url = str(link)[str(link).find('https://pureportal.coventry.ac.uk/en/persons/'):].split('\"')\n",
    "            Links.append(url[0])\n",
    "            \n",
    "        # Check if there is a next page link\n",
    "        next_link_element = driver.find_element(By.CSS_SELECTOR, \".nextLink\")\n",
    "        next_link_enabled = next_link_element.is_enabled()\n",
    "        \n",
    "        if not next_link_enabled:\n",
    "            break\n",
    "        \n",
    "        # Click on Next button to visit next page\n",
    "        driver.execute_script(\"arguments[0].click();\", next_link_element)\n",
    "        time.sleep(6)\n",
    "        \n",
    "    print(\"Crawler has found\", len(Links), \"pureportal profiles\")\n",
    "    write_authors(Links, 'C:/Users/pc/Downloads/7071CEM-main/7071CEM-main/Authors_URL.txt')\n",
    "\n",
    "    print(\"Scraping publication data for\", len(Links), \"pureportal profiles...\")\n",
    "    for link in Links:\n",
    "        # Visit each link to get data\n",
    "        time.sleep(1)\n",
    "        driver.get(link)\n",
    "        try:\n",
    "            if driver.find_elements(By.CSS_SELECTOR, \".portal_link.btn-primary.btn-large\"):\n",
    "                element = driver.find_elements(By.CSS_SELECTOR, \".portal_link.btn-primary.btn-large\")\n",
    "                for a in element:\n",
    "                    if \"research output\".lower() in a.text.lower():\n",
    "                        driver.execute_script(\"arguments[0].click();\", a)\n",
    "                        driver.get(driver.current_url)\n",
    "                        # Get name of Author\n",
    "                        name = driver.find_element(By.CSS_SELECTOR, \"div[class='header person-details']>h1\")\n",
    "                        r = requests.get(driver.current_url)\n",
    "                        # Parse all the data via BeautifulSoup\n",
    "                        soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "                        # Extracting publication name, publication URL, date, and CU Authors\n",
    "                        table = soup.find('ul', attrs={'class': 'list-results'})\n",
    "                        if table is not None:\n",
    "                            for row in table.findAll('div', attrs={'class': 'result-container'}):\n",
    "                                data = {}\n",
    "                                data['name'] = row.h3.a.text\n",
    "                                data['pub_url'] = row.h3.a['href']\n",
    "                                date = row.find(\"span\", class_=\"date\")\n",
    "\n",
    "                                rowitem = row.find_all(['div'])\n",
    "                                span = row.find_all(['span'])\n",
    "                                data['cu_author'] = name.text\n",
    "                                data['date'] = date.text\n",
    "                                print(\"Publication Name:\", row.h3.a.text)\n",
    "                                print(\"Publication URL:\", row.h3.a['href'])\n",
    "                                print(\"CU Author:\", name.text)\n",
    "                                print(\"Date:\", date.text)\n",
    "                                print(\"\\n\")\n",
    "                                pub_data.append(data)\n",
    "            else:\n",
    "                # Get name of Author\n",
    "                name = driver.find_element(By.CSS_SELECTOR, \"div[class='header person-details']>h1\")\n",
    "                r = requests.get(link)\n",
    "                # Parse all the data via BeautifulSoup\n",
    "                soup = BeautifulSoup(r.content, 'lxml')\n",
    "                # Extracting publication name, publication URL, date, and CU Authors\n",
    "                table = soup.find('div', attrs={'class': 'relation-list relation-list-publications'})\n",
    "                if table is not None:\n",
    "                    for row in table.findAll('div', attrs={'class': 'result-container'}):\n",
    "                        data = {}\n",
    "                        data[\"name\"] = row.h3.a.text\n",
    "                        data['pub_url'] = row.h3.a['href']\n",
    "                        date = row.find(\"span\", class_=\"date\")\n",
    "                        rowitem = row.find_all(['div'])\n",
    "                        span = row.find_all(['span'])\n",
    "                        data['cu_author'] = name.text\n",
    "                        data['date'] = date.text\n",
    "                        print(\"Publication Name:\", row.h3.a.text)\n",
    "                        print(\"Publication URL:\", row.h3.a['href'])\n",
    "                        print(\"CU Author:\", name.text)\n",
    "                        print(\"Date:\", date.text)\n",
    "                        print(\"\\n\")\n",
    "                        pub_data.append(data)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    print(\"Crawler has scraped data for\", len(pub_data), \"pureportal publications\")\n",
    "    driver.quit()\n",
    "    # Writing all the scraped results in a file with JSON format\n",
    "    with open('C:/Users/pc/Downloads/7071CEM-main/7071CEM-main/scraper_results.json', 'w') as f:\n",
    "        ujson.dump(pub_data, f)\n",
    "\n",
    "\n",
    "initCrawlerScraper('https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acbe83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Set up ChromeOptions\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "chrome_options.add_argument('--incognito')\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "webdriver_service = Service('/path/to/chromedriver')  # Replace with the path to your ChromeDriver executable\n",
    "driver = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n",
    "\n",
    "# Assuming you have initialized the 'driver' instance and have the code for scraping\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "div_elements = driver.find_elements(By.CSS_SELECTOR, \"body > div\")\n",
    "for div in div_elements:\n",
    "    div.click()\n",
    "\n",
    "# Add a delay of 100 milliseconds\n",
    "WebDriverWait(driver, 0.1).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
